# AVATAR_LAB ğŸ­

An AI Avatar Generator Lab that creates realistic lip-synced talking avatars from a static image and an audio clip using GenAI models.

---

## ğŸš€ Features

- ğŸ¤ Convert any audio into a lip-synced avatar video
- ğŸ§  Powered by VITS, SadTalker, GFPGAN, and face detection models
- ğŸ“¦ Real-time video generation
- ğŸ¨ Clean interface (Streamlit/Flask frontend)
- ğŸ”’ Supports `.env` for environment config

---

## ğŸ§° Tech Stack

- **Backend:** Python, Flask
- **AI Models:** SadTalker, GFPGAN, Wav2Lip/VITS
- **Frontend:** Streamlit or HTML/CSS/JS (custom)
- **Other:** Torch, OpenCV, NumPy, FFmpeg

---

## ğŸ“ Project Structure

AVATAR_LAB/
â”‚
â”œâ”€â”€ app.py # Main entry point
â”œâ”€â”€ inference/ # Lip-sync & animation pipeline
â”œâ”€â”€ checkpoints/ # Pretrained model weights (ignored in Git)
â”œâ”€â”€ results/ # Output videos
â”œâ”€â”€ static/ # Frontend assets
â”œâ”€â”€ templates/ # HTML templates (if Flask)
â”œâ”€â”€ requirements.txt # Python dependencies
â””â”€â”€ README.md # Youâ€™re reading it!


---

## âš™ï¸ Setup Instructions
```bash
# Clone the repo
git clone https://github.com/RajashekarReddy7/AVATARLAB.git
cd AVATARLAB

# Create and activate virtual environment
python -m venv venv
venv\Scripts\activate   # On Windows

# Install dependencies
pip install -r requirements.txt

# Download models manually into checkpoints/
# (Follow SadTalker / VITS / GFPGAN model links)

# Run the app
python app.py
